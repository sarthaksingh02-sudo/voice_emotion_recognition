# ğŸ­ Advanced Voice Emotion Recognition System

A comprehensive real-time emotion recognition system using advanced machine learning techniques, featuring multiple testing interfaces and high-performance audio processing.

## ğŸš€ Key Features

### ğŸ¤ **Voice Testing Suite**
- **Manual Voice Testing** - Record and analyze your voice with detailed feedback
- **Batch Testing** - Test multiple emotions in sequence for comprehensive analysis
- **Real-time Recognition** - Live emotion detection with voice activity detection
- **Interactive Interface** - User-friendly testing with visual feedback

### ğŸ§  **Advanced ML Models**
- **Multiple Algorithms**: SVM, Random Forest, Gradient Boosting, MLP Neural Network, Logistic Regression
- **Feature Engineering**: 642 audio features per sample
- **Model Optimization**: Automated hyperparameter tuning and model selection
- **Performance Tracking**: Comprehensive confusion matrices and performance metrics

### ğŸ“Š **Supported Emotions**
- ğŸ˜Š **Happy** - Joyful, excited expressions
- ğŸ˜¢ **Sad** - Sorrowful, melancholic tones
- ğŸ˜  **Angry** - Frustrated, aggressive speech
- ğŸ˜¨ **Fearful** - Scared, worried expressions
- ğŸ¤¢ **Disgust** - Repulsed, disgusted tones
- ğŸ˜ **Neutral** - Calm, normal speech

### ğŸ”§ **Technical Excellence**
- **RAVDESS Dataset**: Professional emotion dataset with 7,442 samples
- **Advanced Feature Extraction**: MFCC, spectral, chroma, mel-spectrogram, and pitch features
- **Real-time Processing**: Sub-second emotion detection
- **Voice Activity Detection**: Smart recording start/stop detection
- **Confidence Scoring**: Probability-based prediction confidence

## ğŸ¯ **Current Performance**
- **Best Model**: SVM with 65% confidence on sample prediction
- **Real-time Processing**: < 1 second per prediction
- **Feature Extraction**: 642 features per audio sample
- **Voice Activity Detection**: Automatic speech detection

## ğŸ› ï¸ **Installation**

### Prerequisites
```bash
pip install torch torchvision torchaudio
pip install numpy pandas scikit-learn
pip install librosa soundfile pyaudio
pip install matplotlib seaborn
pip install joblib
```

### Quick Setup
```bash
git clone https://github.com/sarthaksingh02-sudo/emotion_recognition_system.git
cd emotion_recognition_system
pip install -r requirements.txt
```

## ğŸ™ï¸ **Voice Testing Options**

### 1. **Simple Voice Test** - Quick Single Test
```bash
python simple_voice_test.py
```
- Records 3 seconds of audio
- Instant emotion analysis
- Detailed confidence breakdown
- Perfect for quick testing

### 2. **Batch Voice Test** - Comprehensive Testing
```bash
python batch_voice_test.py
```
- Tests 6 different emotions in sequence
- Guided emotion prompts
- Accuracy summary and statistics
- Ideal for thorough evaluation

### 3. **Manual Voice Test** - Advanced Interface
```bash
python manual_voice_test.py
```
- Interactive testing interface
- Manual or continuous recording modes
- Detailed analysis with all emotion probabilities
- Recording management options

### 4. **Real-time Recognition**
```bash
python improved_real_time.py
```
- Voice Activity Detection
- Continuous emotion monitoring
- Live feedback with emojis

## ğŸ“ˆ **System Testing & Validation**

### Quick System Check
```bash
python test_final.py          # Complete pipeline test
python system_readiness_test.py  # System readiness validation
python comprehensive_test.py     # Detailed performance analysis
```

### Model Training
```bash
python train_model.py         # Train new models
python model_diagnostic.py    # Model performance diagnostics
```

## ğŸ“ **Project Structure**

```
emotion_recognition_system/
â”œâ”€â”€ ğŸ¤ Voice Testing Suite
â”‚   â”œâ”€â”€ simple_voice_test.py      # Quick single voice test
â”‚   â”œâ”€â”€ batch_voice_test.py       # Comprehensive batch testing
â”‚   â”œâ”€â”€ manual_voice_test.py      # Advanced interactive testing
â”‚   â””â”€â”€ improved_real_time.py     # Real-time recognition
â”œâ”€â”€ ğŸ§  Core System
â”‚   â”œâ”€â”€ train_model.py            # Model training pipeline
â”‚   â”œâ”€â”€ model_trainer.py          # ML model implementations
â”‚   â”œâ”€â”€ audio_processor.py        # Audio feature extraction
â”‚   â”œâ”€â”€ data_parser.py            # RAVDESS dataset parser
â”‚   â””â”€â”€ config.py                 # Configuration settings
â”œâ”€â”€ ğŸ”§ Testing & Validation
â”‚   â”œâ”€â”€ test_final.py             # Complete pipeline test
â”‚   â”œâ”€â”€ system_readiness_test.py  # System validation
â”‚   â”œâ”€â”€ comprehensive_test.py     # Performance analysis
â”‚   â””â”€â”€ model_diagnostic.py       # Model diagnostics
â”œâ”€â”€ ğŸ“Š Generated Assets
â”‚   â”œâ”€â”€ *.png                     # Confusion matrices
â”‚   â”œâ”€â”€ models/                   # Trained models
â”‚   â”œâ”€â”€ data/AudioWAV/            # RAVDESS dataset
â”‚   â””â”€â”€ recordings/               # Voice test recordings
â””â”€â”€ ğŸ“ Documentation
    â”œâ”€â”€ README.md                 # This file
    â””â”€â”€ requirements.txt          # Dependencies
```

## ğŸ§  **Model Architecture**

### Feature Extraction Pipeline
1. **MFCC Features** (52): Mel-frequency cepstral coefficients with statistical measures
2. **Spectral Features** (24): Spectral centroid, rolloff, bandwidth, zero-crossing rate
3. **Chroma Features** (48): Chroma vector with statistical measures
4. **Mel-Spectrogram** (512): Mel-scale spectrogram with statistical measures
5. **Pitch Features** (6): Fundamental frequency characteristics
6. **Total**: 642 features per audio sample

### Available Models
| Model | Type | Status |
|-------|------|--------|
| **SVM** | Support Vector Machine | âœ… Production Ready |
| **Random Forest** | Ensemble Method | âœ… Production Ready |
| **Gradient Boosting** | Ensemble Method | âœ… Production Ready |
| **MLP Neural Network** | Deep Learning | âœ… Production Ready |
| **Logistic Regression** | Linear Model | âœ… Production Ready |

## ğŸ“Š **Performance Metrics**

### Model Comparison (Latest Results)
| Model | Primary Metric | Confidence Range |
|-------|---------------|------------------|
| **SVM** | 65% confidence | High (0.6-0.8) |
| **Random Forest** | Ensemble accuracy | Medium (0.4-0.7) |
| **Gradient Boosting** | Boosted performance | Medium (0.4-0.6) |
| **MLP Neural Network** | Deep learning | Variable (0.3-0.7) |
| **Logistic Regression** | Linear baseline | Low-Medium (0.3-0.6) |

### Confusion Matrices Available
- âœ… SVM Enhanced Confusion Matrix
- âœ… Random Forest Enhanced Confusion Matrix
- âœ… Gradient Boosting Enhanced Confusion Matrix
- âœ… MLP Neural Network Confusion Matrix
- âœ… Logistic Regression Enhanced Confusion Matrix
- âœ… Extra Trees Enhanced Confusion Matrix

## ğŸ¤ **Voice Testing Guide**

### For Best Results:
1. **Environment**: Use a quiet room with minimal background noise
2. **Microphone**: Ensure your microphone is working and positioned correctly
3. **Expression**: Clearly express the intended emotion in your voice
4. **Duration**: Speak for 2-3 seconds with emotional content
5. **Clarity**: Speak clearly and at normal volume

### Emotion Testing Tips:
- **ğŸ˜Š Happy**: Sound joyful, excited, upbeat
- **ğŸ˜¢ Sad**: Speak slowly, with a lower tone, sound melancholic
- **ğŸ˜  Angry**: Use a louder, sharper tone, sound frustrated
- **ğŸ˜¨ Fearful**: Sound worried, anxious, with a shaky voice
- **ğŸ¤¢ Disgust**: Express repulsion, sound disgusted
- **ğŸ˜ Neutral**: Speak normally, calm and balanced

## ğŸ”§ **Configuration**

Edit `config.py` to customize:
- Audio sampling rate and processing parameters
- Feature extraction settings
- Model training parameters
- Real-time processing settings

## ğŸ“Š **Dataset Information**

### RAVDESS Dataset
- **Total Samples**: 7,442 audio files
- **Actors**: 24 professional actors
- **Emotions**: 8 different emotions
- **Quality**: Professional studio recordings
- **Format**: 16-bit WAV files, 22050 Hz sampling rate

### Data Distribution
- **Training**: 80% of samples
- **Testing**: 20% of samples
- **Validation**: Cross-validation during training
- **Preprocessing**: Noise reduction, normalization, feature extraction

## ğŸš€ **Future Improvements**

- [ ] **Web Interface**: Browser-based emotion testing
- [ ] **Mobile App**: Android/iOS emotion recognition
- [ ] **API Service**: RESTful API for integration
- [ ] **Multi-language Support**: Support for different languages
- [ ] **Transformer Models**: State-of-the-art transformer architecture
- [ ] **Real-time Visualization**: Live audio waveform and emotion tracking
- [ ] **Batch Processing**: Process multiple files simultaneously
- [ ] **Model Ensemble**: Combine multiple models for better accuracy

## ğŸ¤ **Contributing**

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“œ **License**

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ™ **Acknowledgments**

- **RAVDESS Dataset**: Ryerson Audio-Visual Database of Emotional Speech and Song
- **PyTorch Team**: Deep learning framework
- **Librosa**: Audio processing library
- **Scikit-learn**: Machine learning library
- **PyAudio**: Real-time audio I/O library

## ğŸ“§ **Contact & Support**

For questions, issues, or contributions:
- **GitHub Issues**: [Create an issue](https://github.com/sarthaksingh02-sudo/emotion_recognition_system/issues)
- **Discussions**: [Join the discussion](https://github.com/sarthaksingh02-sudo/emotion_recognition_system/discussions)

---

**ğŸ­ Made with â¤ï¸ for advancing emotion recognition technology**

*"Understanding emotions through voice - bridging the gap between human expression and machine learning."*
